import torch
from faster_whisper import WhisperModel


class ASRModel:
    """
    ‡∏ï‡∏±‡∏ß‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÇ‡∏°‡πÄ‡∏î‡∏• Faster-Whisper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö Local
    """

    def __init__(self, model_size: str = "distil-large-v3"):
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô GPU (RTX 5090) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.compute_type = "float16" if self.device == "cuda" else "int8"

        print(f"üéôÔ∏è ASR: Loading model '{model_size}' on {self.device}...")
        self.model = WhisperModel(
            model_size, device=self.device, compute_type=self.compute_type
        )
        print("‚úì ASR Model loaded.")

    def transcribe(self, audio_path_or_ndarray):
        """
        ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏õ‡πá‡∏ô Text
        """
        segments, info = self.model.transcribe(audio_path_or_ndarray, beam_size=5)
        text = " ".join([segment.text for segment in segments])
        return text.strip()
