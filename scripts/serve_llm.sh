#!/bin/bash
# scripts/serve_llm.sh

MODE=${1:-vllm}
echo "üß† Starting Model Host with Tool Support: Qwen/Qwen3-8B"

if [ "$MODE" == "vllm" ]; then
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° --enable-auto-tool-choice ‡πÅ‡∏•‡∏∞ --tool-call-parser ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö vLLM
    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ parser 'hermes' ‡∏´‡∏£‡∏∑‡∏≠ 'tool_calling' (‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô vLLM)
    uv run python -m vllm.entrypoints.openai.api_server \
        --model "Qwen/Qwen3-8B" \
        --served-model-name "qwen3-8b" \
        --port 8000 \
        --max-model-len 8192 \
        --gpu-memory-utilization 0.7 \
        --quantization bitsandbytes \
        --load-format bitsandbytes \
        --enable-auto-tool-choice \
        --tool-call-parser hermes \
        --trust-remote-code

elif [ "$MODE" == "ollama" ]; then
    export OLLAMA_HOST="0.0.0.0:8000"
    ollama run qwen3:8b
fi
