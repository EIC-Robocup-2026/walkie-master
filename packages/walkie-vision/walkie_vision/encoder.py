import cv2
import numpy as np
import torch
from PIL import Image
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from transformers import BlipForConditionalGeneration, BlipProcessor


class VisionEncoder:
    def __init__(
        self,
        # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏£‡∏∏‡πà‡∏ô Large ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô Captioning ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞
        caption_model="Salesforce/blip-image-captioning-large",
        embed_model="clip-ViT-B-32",
        device="cuda",
    ):
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")

        with tqdm(total=3, desc="üß† Initializing Semantic Encoder (Large)") as pbar:
            pbar.set_postfix_str("Loading BLIP Large Processor")
            self.vqa_processor = BlipProcessor.from_pretrained(caption_model)
            pbar.update(1)

            pbar.set_postfix_str("Loading BLIP Large Model")
            # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÉ‡∏ä‡πâ VRAM ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß Base (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 1.8GB)
            self.vqa_model = BlipForConditionalGeneration.from_pretrained(
                caption_model
            ).to(self.device)
            pbar.update(1)

            pbar.set_postfix_str("Loading CLIP")
            self.embed_model = SentenceTransformer(embed_model, device=self.device)
            pbar.update(1)
            pbar.set_postfix_str("Ready!")

    def generate_caption(self, image: Image.Image) -> str:
        """
        ‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• Large ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        """
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Captioning-Large ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà Prompt ‡πÄ‡∏•‡∏¢‡∏Å‡πá‡πÑ‡∏î‡πâ
        # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏™‡πà‡πÅ‡∏Ñ‡πà "a photography of" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á
        inputs = self.vqa_processor(image, return_tensors="pt").to(self.device)

        # üõ† ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (High Detail Tuning)
        out = self.vqa_model.generate(
            **inputs,
            max_length=80,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            min_length=20,  # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÑ‡∏°‡πà‡∏™‡∏±‡πâ‡∏ô‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
            num_beams=5,  # ‡πÉ‡∏ä‡πâ Beam Search 5 ‡∏ó‡∏≤‡∏á
            repetition_penalty=1.5,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏•‡∏á‡πÇ‡∏ó‡∏©‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥‡πÉ‡∏´‡πâ‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
            no_repeat_ngram_size=3,  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ß‡∏•‡∏µ‡∏ã‡πâ‡∏≥
            early_stopping=True,
        )

        caption = self.vqa_processor.decode(out[0], skip_special_tokens=True)
        return caption.strip()

    def get_image_embedding(self, image: Image.Image) -> list:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå CLIP (512-dim) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Vector Search"""
        embedding = self.embed_model.encode(image)
        return embedding.tolist() if hasattr(embedding, "tolist") else list(embedding)

    def encode_object(self, image_np: np.ndarray) -> tuple[str, list]:
        """‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û NumPy (BGR) -> ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Caption ‡πÅ‡∏•‡∏∞ Embedding"""
        if image_np is None or image_np.size == 0:
            return "invalid_image", [0.0] * 512

        # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏µ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô PIL
        rgb_img = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(rgb_img)

        # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢ (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏à‡∏∞‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡∏°‡∏≤‡∏Å)
        caption = self.generate_caption(pil_img)

        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå
        embedding = self.get_image_embedding(pil_img)

        return caption, embedding

    def encode_batch(
        self, images_np: list[np.ndarray]
    ) -> tuple[list[str], list[list[float]]]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏¥‡πâ‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô (Batch Processing)"""
        if not images_np:
            return [], []

        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏†‡∏≤‡∏û: ‡πÅ‡∏õ‡∏•‡∏á BGR -> RGB ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô PIL Image ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        pil_images = [
            Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) for img in images_np
        ]

        # 2. Batch Captioning (BLIP Large)
        # ‡∏î‡πâ‡∏ß‡∏¢ RTX 5090 ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏±‡∏î batch_size ‡πÑ‡∏î‡πâ‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å (‡πÄ‡∏ä‡πà‡∏ô 16, 32 ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤)
        inputs = self.vqa_processor(images=pil_images, return_tensors="pt").to(
            self.device
        )

        with torch.no_grad():
            out = self.vqa_model.generate(
                **inputs,
                max_length=80,
                num_beams=3,  # ‡∏•‡∏î beam ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î batch
                repetition_penalty=1.5,
                early_stopping=True,
            )

        captions = self.vqa_processor.batch_decode(out, skip_special_tokens=True)
        captions = [c.strip() for c in captions]

        # 3. Batch Embedding (CLIP)
        # SentenceTransformer ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö batching ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏°‡∏ò‡∏≠‡∏î encode
        embeddings = self.embed_model.encode(
            pil_images,
            batch_size=len(pil_images),
            convert_to_numpy=True,
            show_progress_bar=False,
        )

        return captions, embeddings.tolist()
