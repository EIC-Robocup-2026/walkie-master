import cv2
import numpy as np
import torch
from PIL import Image
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from transformers import PaliGemmaForConditionalGeneration, PaliGemmaProcessor


class VisionEncoder:
    def __init__(
        self,
        # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏∏‡πà‡∏ô 3b-mix-224 ‡∏´‡∏£‡∏∑‡∏≠ 3b-mix-448 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
        caption_model="google/paligemma-3b-ft-cococap-448",
        embed_model="clip-ViT-B-32",
        device="cuda",
    ):
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")

        with tqdm(total=3, desc="üß† Initializing PaliGemma Semantic Encoder") as pbar:
            pbar.set_postfix_str("Loading PaliGemma Processor")
            self.vqa_processor = PaliGemmaProcessor.from_pretrained(caption_model)
            pbar.update(1)

            pbar.set_postfix_str("Loading PaliGemma Model")
            # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö bfloat16 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î VRAM (5090 ‡∏™‡∏ö‡∏≤‡∏¢‡∏°‡∏≤‡∏Å)
            self.vqa_model = (
                PaliGemmaForConditionalGeneration.from_pretrained(
                    caption_model, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True
                )
                .to(self.device)
                .eval()
            )
            pbar.update(1)

            pbar.set_postfix_str("Loading CLIP")
            self.embed_model = SentenceTransformer(embed_model, device=self.device)
            pbar.update(1)
            pbar.set_postfix_str("Ready!")

    def generate_caption(self, image: Image.Image) -> str:
        """‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡∏î‡πâ‡∏ß‡∏¢ PaliGemma ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Prompt ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"""
        # PaliGemma ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Prompt ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏≠‡∏Å Task (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å)
        prompt = "caption en\n"

        inputs = self.vqa_processor(text=prompt, images=image, return_tensors="pt").to(
            self.device
        )

        with torch.no_grad():
            output = self.vqa_model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=False,  # ‡πÉ‡∏ä‡πâ Greedy search ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå
            )

        # ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Prompt ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢
        decoded = self.vqa_processor.decode(output[0], skip_special_tokens=True)
        return decoded[len(prompt) :].strip()

    def get_image_embedding(self, image: Image.Image) -> list:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå CLIP (512-dim) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Semantic Search"""
        embedding = self.embed_model.encode(image)
        return embedding.tolist() if hasattr(embedding, "tolist") else list(embedding)

    def encode_object(self, image_np: np.ndarray) -> tuple[str, list]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß (BGR NumPy -> Caption & Embedding)"""
        if image_np is None or image_np.size == 0:
            return "invalid_image", [0.0] * 512

        rgb_img = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(rgb_img)

        caption = self.generate_caption(pil_img)
        embedding = self.get_image_embedding(pil_img)

        return caption, embedding

    def encode_batch(
        self, images_np: list[np.ndarray]
    ) -> tuple[list[str], list[list[float]]]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô ‡∏î‡∏∂‡∏á‡∏û‡∏•‡∏±‡∏á RTX 5090 ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà"""
        if not images_np:
            return [], []

        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏†‡∏≤‡∏û
        pil_images = [
            Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) for img in images_np
        ]

        # 2. Batch Captioning ‡∏Å‡∏±‡∏ö PaliGemma
        prompt = "caption en\n"
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á List ‡∏Ç‡∏≠‡∏á Prompt ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏†‡∏≤‡∏û
        prompts = [prompt] * len(pil_images)

        inputs = self.vqa_processor(
            text=prompts,
            images=pil_images,
            return_tensors="pt",
            padding=True,  # ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Padding ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô Batch
        ).to(self.device)

        with torch.no_grad():
            output = self.vqa_model.generate(
                **inputs, max_new_tokens=100, do_sample=False
            )

        # Decode ‡πÅ‡∏•‡∏∞‡∏•‡∏ö Prompt ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏•‡∏π‡∏Å‡∏†‡∏≤‡∏û
        decoded_outputs = self.vqa_processor.batch_decode(
            output, skip_special_tokens=True
        )
        captions = [d[len(prompt) :].strip() for d in decoded_outputs]

        # 3. Batch Embedding ‡∏Å‡∏±‡∏ö CLIP
        embeddings = self.embed_model.encode(
            pil_images,
            batch_size=len(pil_images),
            convert_to_numpy=True,
            show_progress_bar=False,
        )

        return captions, embeddings.tolist()
