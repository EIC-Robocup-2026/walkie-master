import cv2
import numpy as np
import torch
from PIL import Image
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from transformers import BlipForConditionalGeneration, BlipProcessor


class VisionEncoder:
    def __init__(
        self,
        vqa_model="Salesforce/blip-vqa-base",
        embed_model="clip-ViT-B-32",
        device="cuda",
    ):
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")

        with tqdm(total=3, desc="üß† Initializing Semantic Encoder") as pbar:
            pbar.set_postfix_str("Loading BLIP Processor")
            self.vqa_processor = BlipProcessor.from_pretrained(vqa_model)
            pbar.update(1)

            pbar.set_postfix_str("Loading BLIP Model")
            self.vqa_model = BlipForConditionalGeneration.from_pretrained(vqa_model).to(
                self.device
            )
            pbar.update(1)

            pbar.set_postfix_str("Loading CLIP")
            self.embed_model = SentenceTransformer(embed_model, device=self.device)
            pbar.update(1)
            pbar.set_postfix_str("Ready!")

    def generate_caption(self, image: Image.Image) -> str:
        """‡πÇ‡∏´‡∏°‡∏î‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏û (Image Captioning) - ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ñ‡∏≤‡∏° describe"""
        # ‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà text prompt ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏™‡πà‡πÅ‡∏Ñ‡πà "a photo of" ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏±‡∏ô‡πÇ‡∏´‡∏°‡∏î Captioning
        inputs = self.vqa_processor(image, "a photo of", return_tensors="pt").to(
            self.device
        )

        # ‡∏õ‡∏£‡∏±‡∏ö max_new_tokens ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
        out = self.vqa_model.generate(**inputs, max_new_tokens=50)
        return self.vqa_processor.decode(out[0], skip_special_tokens=True)

    def ask_question(self, image: Image.Image, question: str) -> str:
        """Visual Question Answering (VQA) - ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        inputs = self.vqa_processor(image, question, return_tensors="pt").to(
            self.device
        )
        out = self.vqa_model.generate(**inputs, max_new_tokens=50)
        return self.vqa_processor.decode(out[0], skip_special_tokens=True)

    def get_image_embedding(self, image: Image.Image) -> list:
        embedding = self.embed_model.encode(image)
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô list ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á (SentenceTransformer ‡∏°‡∏±‡∏Å‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô numpy/torch)
        return embedding.tolist() if hasattr(embedding, "tolist") else list(embedding)

    def encode_object(self, image_np: np.ndarray) -> tuple[str, list]:
        """‡∏™‡∏Å‡∏±‡∏î Caption ‡πÅ‡∏•‡∏∞ Vector ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û NumPy"""
        if image_np is None or image_np.size == 0:
            return "empty image", [0.0] * 512

        # 1. ‡πÅ‡∏õ‡∏•‡∏á BGR (OpenCV) ‡πÄ‡∏õ‡πá‡∏ô RGB (PIL)
        rgb_img = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(rgb_img)

        # 2. ‡πÉ‡∏ä‡πâ generate_caption ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏ñ‡∏≤‡∏° describe
        caption = self.generate_caption(pil_img)

        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Embedding
        embedding = self.get_image_embedding(pil_img)

        return caption, embedding
