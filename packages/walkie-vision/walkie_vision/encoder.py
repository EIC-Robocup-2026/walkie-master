import cv2
import numpy as np
import torch
from PIL import Image
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from transformers import BlipForConditionalGeneration, BlipProcessor


class VisionEncoder:
    def __init__(
        self,
        # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏£‡∏∏‡πà‡∏ô Large ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô Captioning ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞
        caption_model="Salesforce/blip-image-captioning-large",
        embed_model="clip-ViT-B-32",
        device="cuda",
    ):
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")

        with tqdm(total=3, desc="üß† Initializing Semantic Encoder (Large)") as pbar:
            pbar.set_postfix_str("Loading BLIP Large Processor")
            self.vqa_processor = BlipProcessor.from_pretrained(caption_model)
            pbar.update(1)

            pbar.set_postfix_str("Loading BLIP Large Model")
            # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÉ‡∏ä‡πâ VRAM ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß Base (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 1.8GB)
            self.vqa_model = BlipForConditionalGeneration.from_pretrained(
                caption_model
            ).to(self.device)
            pbar.update(1)

            pbar.set_postfix_str("Loading CLIP")
            self.embed_model = SentenceTransformer(embed_model, device=self.device)
            pbar.update(1)
            pbar.set_postfix_str("Ready!")

    def generate_caption(self, image: Image.Image) -> str:
        """
        ‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• Large ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        """
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Captioning-Large ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà Prompt ‡πÄ‡∏•‡∏¢‡∏Å‡πá‡πÑ‡∏î‡πâ
        # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏™‡πà‡πÅ‡∏Ñ‡πà "a photography of" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á
        inputs = self.vqa_processor(image, return_tensors="pt").to(self.device)

        # üõ† ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (High Detail Tuning)
        out = self.vqa_model.generate(
            **inputs,
            max_length=80,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            min_length=20,  # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÑ‡∏°‡πà‡∏™‡∏±‡πâ‡∏ô‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
            num_beams=5,  # ‡πÉ‡∏ä‡πâ Beam Search 5 ‡∏ó‡∏≤‡∏á
            repetition_penalty=1.5,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏•‡∏á‡πÇ‡∏ó‡∏©‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥‡πÉ‡∏´‡πâ‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
            no_repeat_ngram_size=3,  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ß‡∏•‡∏µ‡∏ã‡πâ‡∏≥
            early_stopping=True,
        )

        caption = self.vqa_processor.decode(out[0], skip_special_tokens=True)
        return caption.strip()

    def get_image_embedding(self, image: Image.Image) -> list:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå CLIP (512-dim) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Vector Search"""
        embedding = self.embed_model.encode(image)
        return embedding.tolist() if hasattr(embedding, "tolist") else list(embedding)

    def encode_object(self, image_np: np.ndarray) -> tuple[str, list]:
        """‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û NumPy (BGR) -> ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Caption ‡πÅ‡∏•‡∏∞ Embedding"""
        if image_np is None or image_np.size == 0:
            return "invalid_image", [0.0] * 512

        # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏µ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô PIL
        rgb_img = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(rgb_img)

        # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢ (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏à‡∏∞‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡∏°‡∏≤‡∏Å)
        caption = self.generate_caption(pil_img)

        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå
        embedding = self.get_image_embedding(pil_img)

        return caption, embedding
