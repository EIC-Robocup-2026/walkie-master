#!/bin/bash

# serve_llm.sh: ‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Host Qwen3-8B
# ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö: walkie-master/scripts/

MODE=${1:-vllm}

echo "üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£ Host ‡πÇ‡∏°‡πÄ‡∏î‡∏•: Qwen/Qwen3-8B"

if [ "$MODE" == "vllm" ]; then
    # ‡πÉ‡∏ä‡πâ vLLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ö‡∏ô RTX 5090
    python3 -m vllm.entrypoints.openai.api_server \
        --model "Qwen/Qwen3-8B" \
        --served-model-name "qwen3-8b" \
        --port 8000 \
        --gpu-memory-utilization 0.8 \
        --dtype float16 \
        --trust-remote-code

elif [ "$MODE" == "ollama" ]; then
    # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ Ollama ‡πÉ‡∏´‡πâ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ pull model ‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß
    ollama run qwen3:8b
fi
